# Bekalu - AI Book Assistant

Bekalu is AI-based conversational assistant that allows users to interact with him and receive recommendations for books on various topics. Bekalu's Lllama Model version is built using the Llama-2B model, which is a transformer-based language model developed by Meta. It uses Poe as remote infernce server to handle the model inference.

## Installation

1. Install the required dependencies using pip:

   ```bash
   pip install -r requirements.txt
   ```

   This will install all the necessary Python packages specified in the `requirements.txt` file.

## Usage

1. Run the FastAPI server:

   ```bash
   uvicorn assistant.api:app --reload (from the root directory of the project)
   uvicorn api:app --reload (from the assistant directory)
   ```

   This will start the FastAPI Development server, and the API will be accessible at `http://localhost:8000`. You can remove the --reload flag if you don't want the server to reload automatically on code changes.
   This will trigger the Model (2.2GB) to download (if not already downloaded) and load the model into memory. This will take some time.

2. Send a POST request to `http://localhost:8000/chat/` with a JSON body containing the user's message:

   ```json
   {
     "msg": "Hello, Bekalu!"
   }
   ```

   Replace `"Hello, Bekalu!"` with your message. The API will respond with a message generated by the chatbot.

## API Endpoints

### `POST /chat/`

- **Description**: Receive a response from the Bekalu assistant based on the user's input message.
- **Request Body**:
  - `msg`: The user's message (string).
- **Response**:
  - `response`: The Bekalu assistant's response (string).
  - `msg`: The user's message (string).

## Additional Information

- Bekalu is the name of the assistant.
- Due to payment restrictions in Ethiopia, we faced challenges with deploying the model, hence the remote inference setup.
- Bekalu's purpose is to contribute to UN Sustainable Development Goal #4 Quality Education by helping users discover books on various topics.
- The chatbot model is loaded at startup for faster response times.
- Caching is implemented for better performance using LRU caching.
- Error handling is in place to gracefully handle unexpected errors and provide appropriate responses.
